{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem statement came from a HackerEarth challenge: \"Predict the Happiness\". The objective is to  use 2-layered fully connected/Dense Neural network model to predict whether the hotel reviews at TripAdvisor site are positive sentiment or negative sentiment.\n",
    "\n",
    "References : \n",
    "https://appliedmachinelearning.wordpress.com/2017/12/21/predict-the-happiness-on-tripadvisor-reviews-using-dense-neural-network-with-keras-hackerearth-challenge/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dataset description:\n",
    "\n",
    "- User_ID :: unique ID of the customer\n",
    "\n",
    "- Description :: description of the review posted\n",
    "\n",
    "- Browser_Used :: browser used to post the review\n",
    "\n",
    "- Device_Used :: device used to post the review\n",
    "\n",
    "- Is_Response :: target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in only the desription variable and the is_response variable. This is a 2 class sentiment analysis problem where we have to tell if the customer is happy or not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps :\n",
    "- Preprocess data \n",
    "- Extract features \n",
    "- Build model\n",
    "- Train model\n",
    "- Test performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to read the csv file and prepare the dataset. Further, converting the Is_Response variable to binary 0/1 using LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    data = pd.read_csv(file_path, sep=',')\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    data['Is_Response'] = label_encoder.fit_transform(data['Is_Response'])\n",
    "    \n",
    "    for i in range(0, len(data['Description'])):\n",
    "        feature = data['Description'][i]\n",
    "        features.append(feature)\n",
    "        label = data['Is_Response'][i]\n",
    "        labels.append(label)\n",
    "        \n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return features, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here words are the features. So we use bag-of-words model to create feature vector. \n",
    "\n",
    "- Make a dictionary having word - index tuples. Order not important. \n",
    "- Convert words in the review into word-index array for each review and save it in the global array. \n",
    "- The global array will become the feature matrix with number of colums equal to size of vocab (here limiting it to 10000). It will be a sparse matrix with 1 at the place if that word is present in the sparse matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text2indexarray(text):\n",
    "    return [dictionary[word] for word in text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model: We use a 2 layer NN with 2 nodes at the output for targets happy and unhappy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(max_words,), activation='elu')) ## Exponential linear unit\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = preprocess(\"train.csv\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(features)\n",
    "\n",
    "dictionary = tokenizer.word_index  ## returns a word-index tuple dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vocabulary dictionary\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words in the review to indices \n",
    "allWordIndices = []  ## global word matrix \n",
    "\n",
    "for num, text in enumerate(features):\n",
    "    wordIndices = convert_text2indexarray(text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordIndices = np.asarray(allWordIndices)\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "\n",
    "labels = keras.utils.to_categorical(labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35038 samples, validate on 3894 samples\n",
      "Epoch 1/15\n",
      "35038/35038 [==============================] - 33s 956us/step - loss: 0.3984 - acc: 0.8218 - val_loss: 0.3192 - val_acc: 0.8629\n",
      "Epoch 2/15\n",
      "35038/35038 [==============================] - 31s 897us/step - loss: 0.3163 - acc: 0.8707 - val_loss: 0.3026 - val_acc: 0.8706\n",
      "Epoch 3/15\n",
      "35038/35038 [==============================] - 30s 857us/step - loss: 0.2976 - acc: 0.8771 - val_loss: 0.2961 - val_acc: 0.8790\n",
      "Epoch 4/15\n",
      "35038/35038 [==============================] - 30s 846us/step - loss: 0.2856 - acc: 0.8853 - val_loss: 0.2921 - val_acc: 0.8801\n",
      "Epoch 5/15\n",
      "35038/35038 [==============================] - 32s 912us/step - loss: 0.2757 - acc: 0.8876 - val_loss: 0.2918 - val_acc: 0.8790\n",
      "Epoch 6/15\n",
      "35038/35038 [==============================] - 32s 918us/step - loss: 0.2707 - acc: 0.8912 - val_loss: 0.2924 - val_acc: 0.8803\n",
      "Epoch 7/15\n",
      "35038/35038 [==============================] - 35s 991us/step - loss: 0.2620 - acc: 0.8944 - val_loss: 0.2931 - val_acc: 0.8808\n",
      "Epoch 8/15\n",
      "35038/35038 [==============================] - 30s 869us/step - loss: 0.2577 - acc: 0.8953 - val_loss: 0.2910 - val_acc: 0.8811\n",
      "Epoch 9/15\n",
      "35038/35038 [==============================] - 30s 860us/step - loss: 0.2534 - acc: 0.8992 - val_loss: 0.2949 - val_acc: 0.8821\n",
      "Epoch 10/15\n",
      "35038/35038 [==============================] - 29s 842us/step - loss: 0.2491 - acc: 0.8991 - val_loss: 0.2957 - val_acc: 0.8798\n",
      "Epoch 11/15\n",
      "35038/35038 [==============================] - 30s 846us/step - loss: 0.2467 - acc: 0.9014 - val_loss: 0.2984 - val_acc: 0.8801\n",
      "Epoch 12/15\n",
      "35038/35038 [==============================] - 30s 855us/step - loss: 0.2425 - acc: 0.9013 - val_loss: 0.2997 - val_acc: 0.8780\n",
      "Epoch 13/15\n",
      "35038/35038 [==============================] - 30s 853us/step - loss: 0.2383 - acc: 0.9025 - val_loss: 0.3033 - val_acc: 0.8816\n",
      "Epoch 14/15\n",
      "35038/35038 [==============================] - 32s 906us/step - loss: 0.2367 - acc: 0.9033 - val_loss: 0.3028 - val_acc: 0.8837\n",
      "Epoch 15/15\n",
      "35038/35038 [==============================] - 31s 897us/step - loss: 0.2334 - acc: 0.9056 - val_loss: 0.3039 - val_acc: 0.8803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x104dbca90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(train_x, labels, batch_size=32, epochs=15, verbose=1, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model \n",
    "model_json = model.to_json()\n",
    "\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['happy', 'not_happy']\n",
    "\n",
    "# Load dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)\n",
    "\n",
    "# Load json model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "model = keras.models.model_from_json(loaded_model_json)\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "length = len(test_data['Description'])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    words = text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "for i in range (0, length):\n",
    "    feature = test_data['Description'][i]\n",
    "    test_array = convert_text_to_index_array(feature)\n",
    "    \n",
    "    input = tokenizer.sequences_to_matrix([test_array], mode='binary')\n",
    "    \n",
    "    pred = model.predict(input)\n",
    "    \n",
    "    y_pred.append(labels[np.argmax(pred)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_data = {'User_ID':test_data['User_ID'], 'Is_Response':y_pred}\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns=['User_ID', 'Is_Response'])\n",
    "\n",
    "df.to_csv('results.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
